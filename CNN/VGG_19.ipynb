{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwD_rORBC_vI",
        "outputId": "3e337462-be7a-4d38-a270-4c3db9ddd728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms,models\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n"
      ],
      "metadata": {
        "id": "sfs6mgisDILr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data transformations (normalization and data augmentation)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224 (VGG input size)\n",
        "    transforms.ToTensor(),          # Convert image to PyTorch Tensor\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define data loaders for training and testing\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4oxUrpGFr2A",
        "outputId": "baec60d0-14b5-48c1-dfbb-15bef600d530"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class VGG19(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(VGG19, self).__init__()\n",
        "\n",
        "        # Feature extraction layers: Convolutional and pooling layers\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # 3 input channels, 64 output channels, 3x3 kernel, 1 padding\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling with 2x2 kernel and stride 2\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        # Pooling Layer\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(7, 7))\n",
        "\n",
        "        # Fully connected layers for classification\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),  # 512 channels, 7x7 spatial dimensions after max pooling\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),  # Dropout layer with 0.5 dropout probability\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes),  # Output layer with 'num_classes' output units\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)  # Pass input through the feature extractor layers\n",
        "        x= self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
        "        x = self.classifier(x)  # Pass flattened output through the classifier layers\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "EjEOhvTxDNmB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = VGG19(num_classes=10)  # CIFAR-10 has 10 classes\n",
        "model = model.cuda()  # Move model to GPU if available\n",
        "\n",
        "# Print the model summary\n",
        "summary(model, (3, 224, 224))\n",
        "\n",
        "# Define the loss function (cross-entropy loss for classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer (Adam or SGD)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can also use optim.SGD\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oJzC_3-DU2P",
        "outputId": "e76c6d58-af39-4617-b198-2d1b2f783709"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
            "              ReLU-2         [-1, 64, 224, 224]               0\n",
            "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
            "              ReLU-4         [-1, 64, 224, 224]               0\n",
            "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
            "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
            "              ReLU-7        [-1, 128, 112, 112]               0\n",
            "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
            "              ReLU-9        [-1, 128, 112, 112]               0\n",
            "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
            "             ReLU-12          [-1, 256, 56, 56]               0\n",
            "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-14          [-1, 256, 56, 56]               0\n",
            "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-16          [-1, 256, 56, 56]               0\n",
            "           Conv2d-17          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-18          [-1, 256, 56, 56]               0\n",
            "        MaxPool2d-19          [-1, 256, 28, 28]               0\n",
            "           Conv2d-20          [-1, 512, 28, 28]       1,180,160\n",
            "             ReLU-21          [-1, 512, 28, 28]               0\n",
            "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-23          [-1, 512, 28, 28]               0\n",
            "           Conv2d-24          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-25          [-1, 512, 28, 28]               0\n",
            "           Conv2d-26          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-27          [-1, 512, 28, 28]               0\n",
            "        MaxPool2d-28          [-1, 512, 14, 14]               0\n",
            "AdaptiveAvgPool2d-29            [-1, 512, 7, 7]               0\n",
            "           Linear-30                 [-1, 4096]     102,764,544\n",
            "             ReLU-31                 [-1, 4096]               0\n",
            "          Dropout-32                 [-1, 4096]               0\n",
            "           Linear-33                 [-1, 4096]      16,781,312\n",
            "             ReLU-34                 [-1, 4096]               0\n",
            "          Dropout-35                 [-1, 4096]               0\n",
            "           Linear-36                   [-1, 10]          40,970\n",
            "================================================================\n",
            "Total params: 130,171,978\n",
            "Trainable params: 130,171,978\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 232.36\n",
            "Params size (MB): 496.57\n",
            "Estimated Total Size (MB): 729.50\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained VGG19 model\n",
        "model = models.vgg19(pretrained=True)\n",
        "\n",
        "# Modify the classifier for your dataset (e.g., 10 classes for CIFAR-10)\n",
        "model.classifier[6] = nn.Linear(4096, 10)\n",
        "\n",
        "# Freeze all layers except the classifier\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd69c9hrPGkM",
        "outputId": "c47fea7b-d141-4b7b-a7ee-5febe783a83d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:06<00:00, 84.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx,(inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (batch_idx + 1) % 10 == 0:  # Print every 10 batches\n",
        "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Finished Training\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvRex--3F_hp",
        "outputId": "b34409d8-662a-4392-fd92-721dbd7afe22"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Batch [10/782], Loss: 0.8863\n",
            "Epoch [1/10], Batch [20/782], Loss: 0.6470\n",
            "Epoch [1/10], Batch [30/782], Loss: 0.7139\n",
            "Epoch [1/10], Batch [40/782], Loss: 0.7459\n",
            "Epoch [1/10], Batch [50/782], Loss: 0.8861\n",
            "Epoch [1/10], Batch [60/782], Loss: 0.6194\n",
            "Epoch [1/10], Batch [70/782], Loss: 1.1164\n",
            "Epoch [1/10], Batch [80/782], Loss: 0.6780\n",
            "Epoch [1/10], Batch [90/782], Loss: 0.9033\n",
            "Epoch [1/10], Batch [100/782], Loss: 0.8752\n",
            "Epoch [1/10], Batch [110/782], Loss: 0.6276\n",
            "Epoch [1/10], Batch [120/782], Loss: 0.8578\n",
            "Epoch [1/10], Batch [130/782], Loss: 0.9184\n",
            "Epoch [1/10], Batch [140/782], Loss: 1.1707\n",
            "Epoch [1/10], Batch [150/782], Loss: 0.7914\n",
            "Epoch [1/10], Batch [160/782], Loss: 0.7372\n",
            "Epoch [1/10], Batch [170/782], Loss: 0.4947\n",
            "Epoch [1/10], Batch [180/782], Loss: 0.7922\n",
            "Epoch [1/10], Batch [190/782], Loss: 0.8658\n",
            "Epoch [1/10], Batch [200/782], Loss: 0.5676\n",
            "Epoch [1/10], Batch [210/782], Loss: 0.9345\n",
            "Epoch [1/10], Batch [220/782], Loss: 0.6746\n",
            "Epoch [1/10], Batch [230/782], Loss: 0.6441\n",
            "Epoch [1/10], Batch [240/782], Loss: 0.4472\n",
            "Epoch [1/10], Batch [250/782], Loss: 1.0126\n",
            "Epoch [1/10], Batch [260/782], Loss: 0.4966\n",
            "Epoch [1/10], Batch [270/782], Loss: 0.3548\n",
            "Epoch [1/10], Batch [280/782], Loss: 0.8106\n",
            "Epoch [1/10], Batch [290/782], Loss: 0.5250\n",
            "Epoch [1/10], Batch [300/782], Loss: 0.5696\n",
            "Epoch [1/10], Batch [310/782], Loss: 0.4973\n",
            "Epoch [1/10], Batch [320/782], Loss: 0.3765\n",
            "Epoch [1/10], Batch [330/782], Loss: 0.6403\n",
            "Epoch [1/10], Batch [340/782], Loss: 0.8265\n",
            "Epoch [1/10], Batch [350/782], Loss: 0.8159\n",
            "Epoch [1/10], Batch [360/782], Loss: 0.7893\n",
            "Epoch [1/10], Batch [370/782], Loss: 0.4947\n",
            "Epoch [1/10], Batch [380/782], Loss: 0.7002\n",
            "Epoch [1/10], Batch [390/782], Loss: 0.7568\n",
            "Epoch [1/10], Batch [400/782], Loss: 0.9305\n",
            "Epoch [1/10], Batch [410/782], Loss: 0.5011\n",
            "Epoch [1/10], Batch [420/782], Loss: 0.3570\n",
            "Epoch [1/10], Batch [430/782], Loss: 0.6879\n",
            "Epoch [1/10], Batch [440/782], Loss: 0.8848\n",
            "Epoch [1/10], Batch [450/782], Loss: 0.7452\n",
            "Epoch [1/10], Batch [460/782], Loss: 0.8022\n",
            "Epoch [1/10], Batch [470/782], Loss: 0.4081\n",
            "Epoch [1/10], Batch [480/782], Loss: 0.4795\n",
            "Epoch [1/10], Batch [490/782], Loss: 0.6060\n",
            "Epoch [1/10], Batch [500/782], Loss: 0.6134\n",
            "Epoch [1/10], Batch [510/782], Loss: 0.9587\n",
            "Epoch [1/10], Batch [520/782], Loss: 0.8816\n",
            "Epoch [1/10], Batch [530/782], Loss: 0.7722\n",
            "Epoch [1/10], Batch [540/782], Loss: 0.6395\n",
            "Epoch [1/10], Batch [550/782], Loss: 0.5139\n",
            "Epoch [1/10], Batch [560/782], Loss: 0.3340\n",
            "Epoch [1/10], Batch [570/782], Loss: 0.6734\n",
            "Epoch [1/10], Batch [580/782], Loss: 0.3991\n",
            "Epoch [1/10], Batch [590/782], Loss: 0.8614\n",
            "Epoch [1/10], Batch [600/782], Loss: 0.8310\n",
            "Epoch [1/10], Batch [610/782], Loss: 0.8095\n",
            "Epoch [1/10], Batch [620/782], Loss: 0.6509\n",
            "Epoch [1/10], Batch [630/782], Loss: 0.5512\n",
            "Epoch [1/10], Batch [640/782], Loss: 0.5973\n",
            "Epoch [1/10], Batch [650/782], Loss: 0.6242\n",
            "Epoch [1/10], Batch [660/782], Loss: 0.5135\n",
            "Epoch [1/10], Batch [670/782], Loss: 0.7184\n",
            "Epoch [1/10], Batch [680/782], Loss: 0.9431\n",
            "Epoch [1/10], Batch [690/782], Loss: 0.7609\n",
            "Epoch [1/10], Batch [700/782], Loss: 0.5429\n",
            "Epoch [1/10], Batch [710/782], Loss: 0.7759\n",
            "Epoch [1/10], Batch [720/782], Loss: 0.5153\n",
            "Epoch [1/10], Batch [730/782], Loss: 0.7442\n",
            "Epoch [1/10], Batch [740/782], Loss: 0.3816\n",
            "Epoch [1/10], Batch [750/782], Loss: 0.7126\n",
            "Epoch [1/10], Batch [760/782], Loss: 0.9965\n",
            "Epoch [1/10], Batch [770/782], Loss: 0.7822\n",
            "Epoch [1/10], Batch [780/782], Loss: 0.6450\n",
            "Epoch [1/10], Loss: 0.6992\n",
            "Epoch [2/10], Batch [10/782], Loss: 0.7069\n",
            "Epoch [2/10], Batch [20/782], Loss: 0.6192\n",
            "Epoch [2/10], Batch [30/782], Loss: 0.6177\n",
            "Epoch [2/10], Batch [40/782], Loss: 0.3686\n",
            "Epoch [2/10], Batch [50/782], Loss: 0.8976\n",
            "Epoch [2/10], Batch [60/782], Loss: 0.4094\n",
            "Epoch [2/10], Batch [70/782], Loss: 0.8326\n",
            "Epoch [2/10], Batch [80/782], Loss: 0.4677\n",
            "Epoch [2/10], Batch [90/782], Loss: 0.5665\n",
            "Epoch [2/10], Batch [100/782], Loss: 0.3150\n",
            "Epoch [2/10], Batch [110/782], Loss: 0.4982\n",
            "Epoch [2/10], Batch [120/782], Loss: 0.5161\n",
            "Epoch [2/10], Batch [130/782], Loss: 0.5350\n",
            "Epoch [2/10], Batch [140/782], Loss: 0.6305\n",
            "Epoch [2/10], Batch [150/782], Loss: 0.4887\n",
            "Epoch [2/10], Batch [160/782], Loss: 0.7118\n",
            "Epoch [2/10], Batch [170/782], Loss: 0.7737\n",
            "Epoch [2/10], Batch [180/782], Loss: 0.4218\n",
            "Epoch [2/10], Batch [190/782], Loss: 0.5590\n",
            "Epoch [2/10], Batch [200/782], Loss: 0.4816\n",
            "Epoch [2/10], Batch [210/782], Loss: 0.3583\n",
            "Epoch [2/10], Batch [220/782], Loss: 0.4613\n",
            "Epoch [2/10], Batch [230/782], Loss: 0.5936\n",
            "Epoch [2/10], Batch [240/782], Loss: 0.4483\n",
            "Epoch [2/10], Batch [250/782], Loss: 0.5775\n",
            "Epoch [2/10], Batch [260/782], Loss: 0.4554\n",
            "Epoch [2/10], Batch [270/782], Loss: 0.3419\n",
            "Epoch [2/10], Batch [280/782], Loss: 0.3780\n",
            "Epoch [2/10], Batch [290/782], Loss: 0.7106\n",
            "Epoch [2/10], Batch [300/782], Loss: 0.4777\n",
            "Epoch [2/10], Batch [310/782], Loss: 0.3849\n",
            "Epoch [2/10], Batch [320/782], Loss: 0.6843\n",
            "Epoch [2/10], Batch [330/782], Loss: 0.8190\n",
            "Epoch [2/10], Batch [340/782], Loss: 0.3008\n",
            "Epoch [2/10], Batch [350/782], Loss: 0.5113\n",
            "Epoch [2/10], Batch [360/782], Loss: 0.9406\n",
            "Epoch [2/10], Batch [370/782], Loss: 0.7740\n",
            "Epoch [2/10], Batch [380/782], Loss: 0.5421\n",
            "Epoch [2/10], Batch [390/782], Loss: 0.3977\n",
            "Epoch [2/10], Batch [400/782], Loss: 0.7108\n",
            "Epoch [2/10], Batch [410/782], Loss: 0.7084\n",
            "Epoch [2/10], Batch [420/782], Loss: 0.7196\n",
            "Epoch [2/10], Batch [430/782], Loss: 0.7220\n",
            "Epoch [2/10], Batch [440/782], Loss: 0.7922\n",
            "Epoch [2/10], Batch [450/782], Loss: 0.4722\n",
            "Epoch [2/10], Batch [460/782], Loss: 0.4302\n",
            "Epoch [2/10], Batch [470/782], Loss: 0.4776\n",
            "Epoch [2/10], Batch [480/782], Loss: 0.4635\n",
            "Epoch [2/10], Batch [490/782], Loss: 0.7936\n",
            "Epoch [2/10], Batch [500/782], Loss: 0.4892\n",
            "Epoch [2/10], Batch [510/782], Loss: 0.5853\n",
            "Epoch [2/10], Batch [520/782], Loss: 0.7430\n",
            "Epoch [2/10], Batch [530/782], Loss: 0.7997\n",
            "Epoch [2/10], Batch [540/782], Loss: 0.3659\n",
            "Epoch [2/10], Batch [550/782], Loss: 0.3939\n",
            "Epoch [2/10], Batch [560/782], Loss: 0.4947\n",
            "Epoch [2/10], Batch [570/782], Loss: 0.5671\n",
            "Epoch [2/10], Batch [580/782], Loss: 0.4536\n",
            "Epoch [2/10], Batch [590/782], Loss: 0.8004\n",
            "Epoch [2/10], Batch [600/782], Loss: 0.5661\n",
            "Epoch [2/10], Batch [610/782], Loss: 0.5622\n",
            "Epoch [2/10], Batch [620/782], Loss: 0.7511\n",
            "Epoch [2/10], Batch [630/782], Loss: 0.8406\n",
            "Epoch [2/10], Batch [640/782], Loss: 0.5835\n",
            "Epoch [2/10], Batch [650/782], Loss: 0.3300\n",
            "Epoch [2/10], Batch [660/782], Loss: 0.3219\n",
            "Epoch [2/10], Batch [670/782], Loss: 0.7594\n",
            "Epoch [2/10], Batch [680/782], Loss: 0.4731\n",
            "Epoch [2/10], Batch [690/782], Loss: 0.6649\n",
            "Epoch [2/10], Batch [700/782], Loss: 0.6314\n",
            "Epoch [2/10], Batch [710/782], Loss: 0.7760\n",
            "Epoch [2/10], Batch [720/782], Loss: 0.6847\n",
            "Epoch [2/10], Batch [730/782], Loss: 0.4939\n",
            "Epoch [2/10], Batch [740/782], Loss: 0.3631\n",
            "Epoch [2/10], Batch [750/782], Loss: 0.4411\n",
            "Epoch [2/10], Batch [760/782], Loss: 0.9150\n",
            "Epoch [2/10], Batch [770/782], Loss: 0.9449\n",
            "Epoch [2/10], Batch [780/782], Loss: 0.8745\n",
            "Epoch [2/10], Loss: 0.5732\n",
            "Epoch [3/10], Batch [10/782], Loss: 0.5586\n",
            "Epoch [3/10], Batch [20/782], Loss: 0.8573\n",
            "Epoch [3/10], Batch [30/782], Loss: 0.3255\n",
            "Epoch [3/10], Batch [40/782], Loss: 0.9050\n",
            "Epoch [3/10], Batch [50/782], Loss: 0.3992\n",
            "Epoch [3/10], Batch [60/782], Loss: 0.4681\n",
            "Epoch [3/10], Batch [70/782], Loss: 0.4251\n",
            "Epoch [3/10], Batch [80/782], Loss: 0.2757\n",
            "Epoch [3/10], Batch [90/782], Loss: 0.2371\n",
            "Epoch [3/10], Batch [100/782], Loss: 0.5481\n",
            "Epoch [3/10], Batch [110/782], Loss: 0.2412\n",
            "Epoch [3/10], Batch [120/782], Loss: 0.2713\n",
            "Epoch [3/10], Batch [130/782], Loss: 0.7444\n",
            "Epoch [3/10], Batch [140/782], Loss: 0.3316\n",
            "Epoch [3/10], Batch [150/782], Loss: 0.2596\n",
            "Epoch [3/10], Batch [160/782], Loss: 0.5900\n",
            "Epoch [3/10], Batch [170/782], Loss: 0.4069\n",
            "Epoch [3/10], Batch [180/782], Loss: 0.3305\n",
            "Epoch [3/10], Batch [190/782], Loss: 0.6275\n",
            "Epoch [3/10], Batch [200/782], Loss: 0.7030\n",
            "Epoch [3/10], Batch [210/782], Loss: 0.3391\n",
            "Epoch [3/10], Batch [220/782], Loss: 0.3100\n",
            "Epoch [3/10], Batch [230/782], Loss: 0.4842\n",
            "Epoch [3/10], Batch [240/782], Loss: 0.3551\n",
            "Epoch [3/10], Batch [250/782], Loss: 0.2637\n",
            "Epoch [3/10], Batch [260/782], Loss: 0.5348\n",
            "Epoch [3/10], Batch [270/782], Loss: 0.3747\n",
            "Epoch [3/10], Batch [280/782], Loss: 0.3945\n",
            "Epoch [3/10], Batch [290/782], Loss: 0.3718\n",
            "Epoch [3/10], Batch [300/782], Loss: 0.4764\n",
            "Epoch [3/10], Batch [310/782], Loss: 0.5119\n",
            "Epoch [3/10], Batch [320/782], Loss: 0.3513\n",
            "Epoch [3/10], Batch [330/782], Loss: 0.3648\n",
            "Epoch [3/10], Batch [340/782], Loss: 0.3942\n",
            "Epoch [3/10], Batch [350/782], Loss: 0.6976\n",
            "Epoch [3/10], Batch [360/782], Loss: 0.7142\n",
            "Epoch [3/10], Batch [370/782], Loss: 0.3918\n",
            "Epoch [3/10], Batch [380/782], Loss: 0.3650\n",
            "Epoch [3/10], Batch [390/782], Loss: 0.6331\n",
            "Epoch [3/10], Batch [400/782], Loss: 0.5468\n",
            "Epoch [3/10], Batch [410/782], Loss: 0.3909\n",
            "Epoch [3/10], Batch [420/782], Loss: 0.2896\n",
            "Epoch [3/10], Batch [430/782], Loss: 0.3393\n",
            "Epoch [3/10], Batch [440/782], Loss: 0.2605\n",
            "Epoch [3/10], Batch [450/782], Loss: 0.5612\n",
            "Epoch [3/10], Batch [460/782], Loss: 0.6382\n",
            "Epoch [3/10], Batch [470/782], Loss: 0.4898\n",
            "Epoch [3/10], Batch [480/782], Loss: 0.6594\n",
            "Epoch [3/10], Batch [490/782], Loss: 0.6362\n",
            "Epoch [3/10], Batch [500/782], Loss: 0.9254\n",
            "Epoch [3/10], Batch [510/782], Loss: 0.4833\n",
            "Epoch [3/10], Batch [520/782], Loss: 0.5102\n",
            "Epoch [3/10], Batch [530/782], Loss: 0.3893\n",
            "Epoch [3/10], Batch [540/782], Loss: 0.5444\n",
            "Epoch [3/10], Batch [550/782], Loss: 0.5989\n",
            "Epoch [3/10], Batch [560/782], Loss: 0.3337\n",
            "Epoch [3/10], Batch [570/782], Loss: 0.2577\n",
            "Epoch [3/10], Batch [580/782], Loss: 0.3334\n",
            "Epoch [3/10], Batch [590/782], Loss: 0.3395\n",
            "Epoch [3/10], Batch [600/782], Loss: 0.8570\n",
            "Epoch [3/10], Batch [610/782], Loss: 0.3071\n",
            "Epoch [3/10], Batch [620/782], Loss: 0.3981\n",
            "Epoch [3/10], Batch [630/782], Loss: 0.4110\n",
            "Epoch [3/10], Batch [640/782], Loss: 1.0544\n",
            "Epoch [3/10], Batch [650/782], Loss: 0.6163\n",
            "Epoch [3/10], Batch [660/782], Loss: 0.6535\n",
            "Epoch [3/10], Batch [670/782], Loss: 0.3427\n",
            "Epoch [3/10], Batch [680/782], Loss: 0.7706\n",
            "Epoch [3/10], Batch [690/782], Loss: 0.4431\n",
            "Epoch [3/10], Batch [700/782], Loss: 0.4714\n",
            "Epoch [3/10], Batch [710/782], Loss: 0.2858\n",
            "Epoch [3/10], Batch [720/782], Loss: 0.3723\n",
            "Epoch [3/10], Batch [730/782], Loss: 0.4731\n",
            "Epoch [3/10], Batch [740/782], Loss: 0.6370\n",
            "Epoch [3/10], Batch [750/782], Loss: 0.5852\n",
            "Epoch [3/10], Batch [760/782], Loss: 0.4555\n",
            "Epoch [3/10], Batch [770/782], Loss: 0.6141\n",
            "Epoch [3/10], Batch [780/782], Loss: 0.4882\n",
            "Epoch [3/10], Loss: 0.4882\n",
            "Epoch [4/10], Batch [10/782], Loss: 0.4517\n",
            "Epoch [4/10], Batch [20/782], Loss: 0.6016\n",
            "Epoch [4/10], Batch [30/782], Loss: 0.4021\n",
            "Epoch [4/10], Batch [40/782], Loss: 0.2805\n",
            "Epoch [4/10], Batch [50/782], Loss: 0.4518\n",
            "Epoch [4/10], Batch [60/782], Loss: 0.2252\n",
            "Epoch [4/10], Batch [70/782], Loss: 0.6210\n",
            "Epoch [4/10], Batch [80/782], Loss: 0.4862\n",
            "Epoch [4/10], Batch [90/782], Loss: 0.2074\n",
            "Epoch [4/10], Batch [100/782], Loss: 0.6986\n",
            "Epoch [4/10], Batch [110/782], Loss: 0.5527\n",
            "Epoch [4/10], Batch [120/782], Loss: 0.6012\n",
            "Epoch [4/10], Batch [130/782], Loss: 0.7002\n",
            "Epoch [4/10], Batch [140/782], Loss: 0.5023\n",
            "Epoch [4/10], Batch [150/782], Loss: 0.3125\n",
            "Epoch [4/10], Batch [160/782], Loss: 0.2869\n",
            "Epoch [4/10], Batch [170/782], Loss: 0.5976\n",
            "Epoch [4/10], Batch [180/782], Loss: 0.7524\n",
            "Epoch [4/10], Batch [190/782], Loss: 0.4857\n",
            "Epoch [4/10], Batch [200/782], Loss: 0.5080\n",
            "Epoch [4/10], Batch [210/782], Loss: 0.1094\n",
            "Epoch [4/10], Batch [220/782], Loss: 0.5097\n",
            "Epoch [4/10], Batch [230/782], Loss: 0.5574\n",
            "Epoch [4/10], Batch [240/782], Loss: 0.2935\n",
            "Epoch [4/10], Batch [250/782], Loss: 0.3501\n",
            "Epoch [4/10], Batch [260/782], Loss: 0.3908\n",
            "Epoch [4/10], Batch [270/782], Loss: 0.2535\n",
            "Epoch [4/10], Batch [280/782], Loss: 0.2693\n",
            "Epoch [4/10], Batch [290/782], Loss: 0.2498\n",
            "Epoch [4/10], Batch [300/782], Loss: 0.1709\n",
            "Epoch [4/10], Batch [310/782], Loss: 0.3735\n",
            "Epoch [4/10], Batch [320/782], Loss: 0.8189\n",
            "Epoch [4/10], Batch [330/782], Loss: 0.1444\n",
            "Epoch [4/10], Batch [340/782], Loss: 0.9308\n",
            "Epoch [4/10], Batch [350/782], Loss: 0.3962\n",
            "Epoch [4/10], Batch [360/782], Loss: 0.3134\n",
            "Epoch [4/10], Batch [370/782], Loss: 0.2622\n",
            "Epoch [4/10], Batch [380/782], Loss: 0.2017\n",
            "Epoch [4/10], Batch [390/782], Loss: 0.3954\n",
            "Epoch [4/10], Batch [400/782], Loss: 0.7979\n",
            "Epoch [4/10], Batch [410/782], Loss: 0.3453\n",
            "Epoch [4/10], Batch [420/782], Loss: 0.2143\n",
            "Epoch [4/10], Batch [430/782], Loss: 0.3254\n",
            "Epoch [4/10], Batch [440/782], Loss: 0.3476\n",
            "Epoch [4/10], Batch [450/782], Loss: 0.5127\n",
            "Epoch [4/10], Batch [460/782], Loss: 0.2668\n",
            "Epoch [4/10], Batch [470/782], Loss: 0.4299\n",
            "Epoch [4/10], Batch [480/782], Loss: 0.2661\n",
            "Epoch [4/10], Batch [490/782], Loss: 0.2314\n",
            "Epoch [4/10], Batch [500/782], Loss: 0.1377\n",
            "Epoch [4/10], Batch [510/782], Loss: 0.6289\n",
            "Epoch [4/10], Batch [520/782], Loss: 0.1639\n",
            "Epoch [4/10], Batch [530/782], Loss: 0.5665\n",
            "Epoch [4/10], Batch [540/782], Loss: 0.5003\n",
            "Epoch [4/10], Batch [550/782], Loss: 0.2272\n",
            "Epoch [4/10], Batch [560/782], Loss: 0.2844\n",
            "Epoch [4/10], Batch [570/782], Loss: 0.4521\n",
            "Epoch [4/10], Batch [580/782], Loss: 0.4378\n",
            "Epoch [4/10], Batch [590/782], Loss: 0.5253\n",
            "Epoch [4/10], Batch [600/782], Loss: 0.4104\n",
            "Epoch [4/10], Batch [610/782], Loss: 0.2745\n",
            "Epoch [4/10], Batch [620/782], Loss: 0.5038\n",
            "Epoch [4/10], Batch [630/782], Loss: 0.3344\n",
            "Epoch [4/10], Batch [640/782], Loss: 0.6675\n",
            "Epoch [4/10], Batch [650/782], Loss: 0.4094\n",
            "Epoch [4/10], Batch [660/782], Loss: 0.4152\n",
            "Epoch [4/10], Batch [670/782], Loss: 0.1779\n",
            "Epoch [4/10], Batch [680/782], Loss: 0.3900\n",
            "Epoch [4/10], Batch [690/782], Loss: 0.5807\n",
            "Epoch [4/10], Batch [700/782], Loss: 0.5720\n",
            "Epoch [4/10], Batch [710/782], Loss: 0.4928\n",
            "Epoch [4/10], Batch [720/782], Loss: 0.4471\n",
            "Epoch [4/10], Batch [730/782], Loss: 0.3266\n",
            "Epoch [4/10], Batch [740/782], Loss: 0.3418\n",
            "Epoch [4/10], Batch [750/782], Loss: 0.4608\n",
            "Epoch [4/10], Batch [760/782], Loss: 0.4105\n",
            "Epoch [4/10], Batch [770/782], Loss: 0.6271\n",
            "Epoch [4/10], Batch [780/782], Loss: 0.4017\n",
            "Epoch [4/10], Loss: 0.4316\n",
            "Epoch [5/10], Batch [10/782], Loss: 0.2346\n",
            "Epoch [5/10], Batch [20/782], Loss: 0.3142\n",
            "Epoch [5/10], Batch [30/782], Loss: 0.4435\n",
            "Epoch [5/10], Batch [40/782], Loss: 0.3610\n",
            "Epoch [5/10], Batch [50/782], Loss: 0.6185\n",
            "Epoch [5/10], Batch [60/782], Loss: 0.3218\n",
            "Epoch [5/10], Batch [70/782], Loss: 0.9171\n",
            "Epoch [5/10], Batch [80/782], Loss: 0.1023\n",
            "Epoch [5/10], Batch [90/782], Loss: 0.1610\n",
            "Epoch [5/10], Batch [100/782], Loss: 0.2858\n",
            "Epoch [5/10], Batch [110/782], Loss: 0.5111\n",
            "Epoch [5/10], Batch [120/782], Loss: 0.2226\n",
            "Epoch [5/10], Batch [130/782], Loss: 0.4225\n",
            "Epoch [5/10], Batch [140/782], Loss: 0.1729\n",
            "Epoch [5/10], Batch [150/782], Loss: 0.2413\n",
            "Epoch [5/10], Batch [160/782], Loss: 0.3435\n",
            "Epoch [5/10], Batch [170/782], Loss: 0.2431\n",
            "Epoch [5/10], Batch [180/782], Loss: 0.6077\n",
            "Epoch [5/10], Batch [190/782], Loss: 0.2140\n",
            "Epoch [5/10], Batch [200/782], Loss: 0.3067\n",
            "Epoch [5/10], Batch [210/782], Loss: 0.6838\n",
            "Epoch [5/10], Batch [220/782], Loss: 0.5683\n",
            "Epoch [5/10], Batch [230/782], Loss: 0.1138\n",
            "Epoch [5/10], Batch [240/782], Loss: 0.4872\n",
            "Epoch [5/10], Batch [250/782], Loss: 0.4047\n",
            "Epoch [5/10], Batch [260/782], Loss: 0.2531\n",
            "Epoch [5/10], Batch [270/782], Loss: 0.5660\n",
            "Epoch [5/10], Batch [280/782], Loss: 0.2551\n",
            "Epoch [5/10], Batch [290/782], Loss: 0.3395\n",
            "Epoch [5/10], Batch [300/782], Loss: 0.3184\n",
            "Epoch [5/10], Batch [310/782], Loss: 0.1985\n",
            "Epoch [5/10], Batch [320/782], Loss: 0.4903\n",
            "Epoch [5/10], Batch [330/782], Loss: 0.3206\n",
            "Epoch [5/10], Batch [340/782], Loss: 0.3001\n",
            "Epoch [5/10], Batch [350/782], Loss: 0.1484\n",
            "Epoch [5/10], Batch [360/782], Loss: 0.2823\n",
            "Epoch [5/10], Batch [370/782], Loss: 0.3191\n",
            "Epoch [5/10], Batch [380/782], Loss: 0.2590\n",
            "Epoch [5/10], Batch [390/782], Loss: 0.3501\n",
            "Epoch [5/10], Batch [400/782], Loss: 0.2650\n",
            "Epoch [5/10], Batch [410/782], Loss: 0.4458\n",
            "Epoch [5/10], Batch [420/782], Loss: 0.8146\n",
            "Epoch [5/10], Batch [430/782], Loss: 0.4792\n",
            "Epoch [5/10], Batch [440/782], Loss: 0.8935\n",
            "Epoch [5/10], Batch [450/782], Loss: 0.3826\n",
            "Epoch [5/10], Batch [460/782], Loss: 0.6844\n",
            "Epoch [5/10], Batch [470/782], Loss: 0.6129\n",
            "Epoch [5/10], Batch [480/782], Loss: 0.5221\n",
            "Epoch [5/10], Batch [490/782], Loss: 0.2277\n",
            "Epoch [5/10], Batch [500/782], Loss: 0.2207\n",
            "Epoch [5/10], Batch [510/782], Loss: 0.4980\n",
            "Epoch [5/10], Batch [520/782], Loss: 0.2055\n",
            "Epoch [5/10], Batch [530/782], Loss: 0.2443\n",
            "Epoch [5/10], Batch [540/782], Loss: 0.2842\n",
            "Epoch [5/10], Batch [550/782], Loss: 0.1677\n",
            "Epoch [5/10], Batch [560/782], Loss: 0.3821\n",
            "Epoch [5/10], Batch [570/782], Loss: 0.3084\n",
            "Epoch [5/10], Batch [580/782], Loss: 0.3226\n",
            "Epoch [5/10], Batch [590/782], Loss: 0.5696\n",
            "Epoch [5/10], Batch [600/782], Loss: 0.7075\n",
            "Epoch [5/10], Batch [610/782], Loss: 0.1567\n",
            "Epoch [5/10], Batch [620/782], Loss: 0.3283\n",
            "Epoch [5/10], Batch [630/782], Loss: 0.4461\n",
            "Epoch [5/10], Batch [640/782], Loss: 0.3047\n",
            "Epoch [5/10], Batch [650/782], Loss: 0.4265\n",
            "Epoch [5/10], Batch [660/782], Loss: 0.3525\n",
            "Epoch [5/10], Batch [670/782], Loss: 0.4693\n",
            "Epoch [5/10], Batch [680/782], Loss: 0.5334\n",
            "Epoch [5/10], Batch [690/782], Loss: 0.3507\n",
            "Epoch [5/10], Batch [700/782], Loss: 0.1854\n",
            "Epoch [5/10], Batch [710/782], Loss: 0.4489\n",
            "Epoch [5/10], Batch [720/782], Loss: 0.3850\n",
            "Epoch [5/10], Batch [730/782], Loss: 0.6619\n",
            "Epoch [5/10], Batch [740/782], Loss: 0.2873\n",
            "Epoch [5/10], Batch [750/782], Loss: 0.3524\n",
            "Epoch [5/10], Batch [760/782], Loss: 0.5708\n",
            "Epoch [5/10], Batch [770/782], Loss: 0.1959\n",
            "Epoch [5/10], Batch [780/782], Loss: 0.5410\n",
            "Epoch [5/10], Loss: 0.3896\n",
            "Epoch [6/10], Batch [10/782], Loss: 0.2379\n",
            "Epoch [6/10], Batch [20/782], Loss: 0.1706\n",
            "Epoch [6/10], Batch [30/782], Loss: 0.2941\n",
            "Epoch [6/10], Batch [40/782], Loss: 0.4368\n",
            "Epoch [6/10], Batch [50/782], Loss: 0.1210\n",
            "Epoch [6/10], Batch [60/782], Loss: 0.3787\n",
            "Epoch [6/10], Batch [70/782], Loss: 0.1595\n",
            "Epoch [6/10], Batch [80/782], Loss: 0.2891\n",
            "Epoch [6/10], Batch [90/782], Loss: 0.1297\n",
            "Epoch [6/10], Batch [100/782], Loss: 0.0859\n",
            "Epoch [6/10], Batch [110/782], Loss: 0.3225\n",
            "Epoch [6/10], Batch [120/782], Loss: 0.1129\n",
            "Epoch [6/10], Batch [130/782], Loss: 0.4208\n",
            "Epoch [6/10], Batch [140/782], Loss: 0.2309\n",
            "Epoch [6/10], Batch [150/782], Loss: 0.1808\n",
            "Epoch [6/10], Batch [160/782], Loss: 0.5934\n",
            "Epoch [6/10], Batch [170/782], Loss: 0.1117\n",
            "Epoch [6/10], Batch [180/782], Loss: 0.1800\n",
            "Epoch [6/10], Batch [190/782], Loss: 0.1874\n",
            "Epoch [6/10], Batch [200/782], Loss: 0.3457\n",
            "Epoch [6/10], Batch [210/782], Loss: 0.2185\n",
            "Epoch [6/10], Batch [220/782], Loss: 0.2579\n",
            "Epoch [6/10], Batch [230/782], Loss: 0.2466\n",
            "Epoch [6/10], Batch [240/782], Loss: 0.3025\n",
            "Epoch [6/10], Batch [250/782], Loss: 0.3901\n",
            "Epoch [6/10], Batch [260/782], Loss: 0.4335\n",
            "Epoch [6/10], Batch [270/782], Loss: 0.2644\n",
            "Epoch [6/10], Batch [280/782], Loss: 0.2118\n",
            "Epoch [6/10], Batch [290/782], Loss: 0.5927\n",
            "Epoch [6/10], Batch [300/782], Loss: 0.2473\n",
            "Epoch [6/10], Batch [310/782], Loss: 0.3520\n",
            "Epoch [6/10], Batch [320/782], Loss: 0.2523\n",
            "Epoch [6/10], Batch [330/782], Loss: 0.3372\n",
            "Epoch [6/10], Batch [340/782], Loss: 0.1161\n",
            "Epoch [6/10], Batch [350/782], Loss: 0.7034\n",
            "Epoch [6/10], Batch [360/782], Loss: 0.3338\n",
            "Epoch [6/10], Batch [370/782], Loss: 0.2695\n",
            "Epoch [6/10], Batch [380/782], Loss: 0.2180\n",
            "Epoch [6/10], Batch [390/782], Loss: 0.1522\n",
            "Epoch [6/10], Batch [400/782], Loss: 0.1318\n",
            "Epoch [6/10], Batch [410/782], Loss: 0.9046\n",
            "Epoch [6/10], Batch [420/782], Loss: 0.3730\n",
            "Epoch [6/10], Batch [430/782], Loss: 0.3726\n",
            "Epoch [6/10], Batch [440/782], Loss: 0.4144\n",
            "Epoch [6/10], Batch [450/782], Loss: 0.5414\n",
            "Epoch [6/10], Batch [460/782], Loss: 0.2259\n",
            "Epoch [6/10], Batch [470/782], Loss: 0.3834\n",
            "Epoch [6/10], Batch [480/782], Loss: 0.3100\n",
            "Epoch [6/10], Batch [490/782], Loss: 0.2153\n",
            "Epoch [6/10], Batch [500/782], Loss: 0.3711\n",
            "Epoch [6/10], Batch [510/782], Loss: 0.8352\n",
            "Epoch [6/10], Batch [520/782], Loss: 0.3072\n",
            "Epoch [6/10], Batch [530/782], Loss: 0.1662\n",
            "Epoch [6/10], Batch [540/782], Loss: 0.1942\n",
            "Epoch [6/10], Batch [550/782], Loss: 0.2605\n",
            "Epoch [6/10], Batch [560/782], Loss: 0.4292\n",
            "Epoch [6/10], Batch [570/782], Loss: 0.1464\n",
            "Epoch [6/10], Batch [580/782], Loss: 0.2102\n",
            "Epoch [6/10], Batch [590/782], Loss: 0.1156\n",
            "Epoch [6/10], Batch [600/782], Loss: 0.4143\n",
            "Epoch [6/10], Batch [610/782], Loss: 0.6082\n",
            "Epoch [6/10], Batch [620/782], Loss: 0.3189\n",
            "Epoch [6/10], Batch [630/782], Loss: 0.4183\n",
            "Epoch [6/10], Batch [640/782], Loss: 0.2731\n",
            "Epoch [6/10], Batch [650/782], Loss: 0.5264\n",
            "Epoch [6/10], Batch [660/782], Loss: 0.1941\n",
            "Epoch [6/10], Batch [670/782], Loss: 0.5825\n",
            "Epoch [6/10], Batch [680/782], Loss: 0.4990\n",
            "Epoch [6/10], Batch [690/782], Loss: 0.7853\n",
            "Epoch [6/10], Batch [700/782], Loss: 0.3607\n",
            "Epoch [6/10], Batch [710/782], Loss: 0.3926\n",
            "Epoch [6/10], Batch [720/782], Loss: 0.1929\n",
            "Epoch [6/10], Batch [730/782], Loss: 0.2959\n",
            "Epoch [6/10], Batch [740/782], Loss: 0.4010\n",
            "Epoch [6/10], Batch [750/782], Loss: 0.3412\n",
            "Epoch [6/10], Batch [760/782], Loss: 0.2620\n",
            "Epoch [6/10], Batch [770/782], Loss: 0.5841\n",
            "Epoch [6/10], Batch [780/782], Loss: 0.2418\n",
            "Epoch [6/10], Loss: 0.3579\n",
            "Epoch [7/10], Batch [10/782], Loss: 0.3364\n",
            "Epoch [7/10], Batch [20/782], Loss: 0.3591\n",
            "Epoch [7/10], Batch [30/782], Loss: 0.3893\n",
            "Epoch [7/10], Batch [40/782], Loss: 0.1960\n",
            "Epoch [7/10], Batch [50/782], Loss: 0.2738\n",
            "Epoch [7/10], Batch [60/782], Loss: 0.5693\n",
            "Epoch [7/10], Batch [70/782], Loss: 0.3328\n",
            "Epoch [7/10], Batch [80/782], Loss: 0.3156\n",
            "Epoch [7/10], Batch [90/782], Loss: 0.2432\n",
            "Epoch [7/10], Batch [100/782], Loss: 0.5365\n",
            "Epoch [7/10], Batch [110/782], Loss: 0.4312\n",
            "Epoch [7/10], Batch [120/782], Loss: 0.0849\n",
            "Epoch [7/10], Batch [130/782], Loss: 0.1493\n",
            "Epoch [7/10], Batch [140/782], Loss: 0.2591\n",
            "Epoch [7/10], Batch [150/782], Loss: 0.3388\n",
            "Epoch [7/10], Batch [160/782], Loss: 0.3607\n",
            "Epoch [7/10], Batch [170/782], Loss: 0.7747\n",
            "Epoch [7/10], Batch [180/782], Loss: 0.2542\n",
            "Epoch [7/10], Batch [200/782], Loss: 0.3970\n",
            "Epoch [7/10], Batch [210/782], Loss: 0.5433\n",
            "Epoch [7/10], Batch [220/782], Loss: 0.3486\n",
            "Epoch [7/10], Batch [230/782], Loss: 0.2424\n",
            "Epoch [7/10], Batch [240/782], Loss: 0.3428\n",
            "Epoch [7/10], Batch [250/782], Loss: 0.3168\n",
            "Epoch [7/10], Batch [260/782], Loss: 0.1516\n",
            "Epoch [7/10], Batch [270/782], Loss: 0.5761\n",
            "Epoch [7/10], Batch [280/782], Loss: 0.3797\n",
            "Epoch [7/10], Batch [290/782], Loss: 0.0628\n",
            "Epoch [7/10], Batch [300/782], Loss: 0.1203\n",
            "Epoch [7/10], Batch [310/782], Loss: 0.1929\n",
            "Epoch [7/10], Batch [320/782], Loss: 0.3186\n",
            "Epoch [7/10], Batch [330/782], Loss: 0.1053\n",
            "Epoch [7/10], Batch [340/782], Loss: 0.1164\n",
            "Epoch [7/10], Batch [350/782], Loss: 0.4437\n",
            "Epoch [7/10], Batch [360/782], Loss: 0.1388\n",
            "Epoch [7/10], Batch [370/782], Loss: 0.5090\n",
            "Epoch [7/10], Batch [380/782], Loss: 0.6575\n",
            "Epoch [7/10], Batch [390/782], Loss: 0.1251\n",
            "Epoch [7/10], Batch [400/782], Loss: 0.3378\n",
            "Epoch [7/10], Batch [410/782], Loss: 0.9237\n",
            "Epoch [7/10], Batch [420/782], Loss: 0.3973\n",
            "Epoch [7/10], Batch [430/782], Loss: 0.3289\n",
            "Epoch [7/10], Batch [440/782], Loss: 0.5095\n",
            "Epoch [7/10], Batch [450/782], Loss: 0.1955\n",
            "Epoch [7/10], Batch [460/782], Loss: 0.2449\n",
            "Epoch [7/10], Batch [470/782], Loss: 0.5214\n",
            "Epoch [7/10], Batch [480/782], Loss: 0.2391\n",
            "Epoch [7/10], Batch [490/782], Loss: 0.2836\n",
            "Epoch [7/10], Batch [500/782], Loss: 0.2444\n",
            "Epoch [7/10], Batch [510/782], Loss: 0.2161\n",
            "Epoch [7/10], Batch [520/782], Loss: 0.2245\n",
            "Epoch [7/10], Batch [530/782], Loss: 0.4405\n",
            "Epoch [7/10], Batch [540/782], Loss: 0.3098\n",
            "Epoch [7/10], Batch [550/782], Loss: 0.3636\n",
            "Epoch [7/10], Batch [560/782], Loss: 0.5998\n",
            "Epoch [7/10], Batch [570/782], Loss: 0.4988\n",
            "Epoch [7/10], Batch [580/782], Loss: 0.3272\n",
            "Epoch [7/10], Batch [590/782], Loss: 0.2929\n",
            "Epoch [7/10], Batch [600/782], Loss: 0.3865\n",
            "Epoch [7/10], Batch [610/782], Loss: 0.2706\n",
            "Epoch [7/10], Batch [620/782], Loss: 0.3087\n",
            "Epoch [7/10], Batch [630/782], Loss: 0.3194\n",
            "Epoch [7/10], Batch [640/782], Loss: 0.4980\n",
            "Epoch [7/10], Batch [650/782], Loss: 0.4686\n",
            "Epoch [7/10], Batch [660/782], Loss: 0.5213\n",
            "Epoch [7/10], Batch [670/782], Loss: 0.3996\n",
            "Epoch [7/10], Batch [680/782], Loss: 0.3786\n",
            "Epoch [7/10], Batch [690/782], Loss: 0.5858\n",
            "Epoch [7/10], Batch [700/782], Loss: 0.3623\n",
            "Epoch [7/10], Batch [710/782], Loss: 0.2271\n",
            "Epoch [7/10], Batch [720/782], Loss: 0.2903\n",
            "Epoch [7/10], Batch [730/782], Loss: 0.3075\n",
            "Epoch [7/10], Batch [740/782], Loss: 0.2904\n",
            "Epoch [7/10], Batch [750/782], Loss: 0.3365\n",
            "Epoch [7/10], Batch [760/782], Loss: 0.2335\n",
            "Epoch [7/10], Batch [770/782], Loss: 0.3329\n",
            "Epoch [7/10], Batch [780/782], Loss: 0.2003\n",
            "Epoch [7/10], Loss: 0.3369\n",
            "Epoch [8/10], Batch [10/782], Loss: 0.2529\n",
            "Epoch [8/10], Batch [20/782], Loss: 0.2671\n",
            "Epoch [8/10], Batch [30/782], Loss: 0.6955\n",
            "Epoch [8/10], Batch [40/782], Loss: 0.2027\n",
            "Epoch [8/10], Batch [50/782], Loss: 0.3035\n",
            "Epoch [8/10], Batch [60/782], Loss: 0.1978\n",
            "Epoch [8/10], Batch [70/782], Loss: 0.1253\n",
            "Epoch [8/10], Batch [80/782], Loss: 0.2087\n",
            "Epoch [8/10], Batch [90/782], Loss: 0.1905\n",
            "Epoch [8/10], Batch [100/782], Loss: 0.2339\n",
            "Epoch [8/10], Batch [110/782], Loss: 0.0998\n",
            "Epoch [8/10], Batch [120/782], Loss: 0.3145\n",
            "Epoch [8/10], Batch [130/782], Loss: 0.0938\n",
            "Epoch [8/10], Batch [140/782], Loss: 0.3575\n",
            "Epoch [8/10], Batch [150/782], Loss: 0.1436\n",
            "Epoch [8/10], Batch [160/782], Loss: 0.6443\n",
            "Epoch [8/10], Batch [170/782], Loss: 0.2146\n",
            "Epoch [8/10], Batch [180/782], Loss: 0.4623\n",
            "Epoch [8/10], Batch [190/782], Loss: 0.4049\n",
            "Epoch [8/10], Batch [200/782], Loss: 0.3817\n",
            "Epoch [8/10], Batch [210/782], Loss: 0.2218\n",
            "Epoch [8/10], Batch [220/782], Loss: 0.1476\n",
            "Epoch [8/10], Batch [230/782], Loss: 0.3528\n",
            "Epoch [8/10], Batch [240/782], Loss: 0.2796\n",
            "Epoch [8/10], Batch [250/782], Loss: 0.3301\n",
            "Epoch [8/10], Batch [260/782], Loss: 0.4935\n",
            "Epoch [8/10], Batch [270/782], Loss: 0.4079\n",
            "Epoch [8/10], Batch [280/782], Loss: 0.2391\n",
            "Epoch [8/10], Batch [290/782], Loss: 0.1764\n",
            "Epoch [8/10], Batch [300/782], Loss: 0.1711\n",
            "Epoch [8/10], Batch [310/782], Loss: 0.1053\n",
            "Epoch [8/10], Batch [320/782], Loss: 0.3387\n",
            "Epoch [8/10], Batch [330/782], Loss: 0.1673\n",
            "Epoch [8/10], Batch [340/782], Loss: 0.1400\n",
            "Epoch [8/10], Batch [350/782], Loss: 0.4062\n",
            "Epoch [8/10], Batch [360/782], Loss: 0.2490\n",
            "Epoch [8/10], Batch [370/782], Loss: 0.1869\n",
            "Epoch [8/10], Batch [380/782], Loss: 0.2988\n",
            "Epoch [8/10], Batch [390/782], Loss: 0.3305\n",
            "Epoch [8/10], Batch [400/782], Loss: 0.5049\n",
            "Epoch [8/10], Batch [410/782], Loss: 0.7276\n",
            "Epoch [8/10], Batch [420/782], Loss: 0.3746\n",
            "Epoch [8/10], Batch [430/782], Loss: 0.4158\n",
            "Epoch [8/10], Batch [440/782], Loss: 0.6588\n",
            "Epoch [8/10], Batch [450/782], Loss: 0.4174\n",
            "Epoch [8/10], Batch [460/782], Loss: 0.1471\n",
            "Epoch [8/10], Batch [470/782], Loss: 0.1934\n",
            "Epoch [8/10], Batch [480/782], Loss: 0.1722\n",
            "Epoch [8/10], Batch [490/782], Loss: 0.4172\n",
            "Epoch [8/10], Batch [500/782], Loss: 0.6389\n",
            "Epoch [8/10], Batch [510/782], Loss: 0.1319\n",
            "Epoch [8/10], Batch [520/782], Loss: 0.2090\n",
            "Epoch [8/10], Batch [530/782], Loss: 0.3437\n",
            "Epoch [8/10], Batch [540/782], Loss: 0.2316\n",
            "Epoch [8/10], Batch [550/782], Loss: 0.5166\n",
            "Epoch [8/10], Batch [560/782], Loss: 0.2522\n",
            "Epoch [8/10], Batch [570/782], Loss: 0.2608\n",
            "Epoch [8/10], Batch [580/782], Loss: 0.3195\n",
            "Epoch [8/10], Batch [590/782], Loss: 0.0785\n",
            "Epoch [8/10], Batch [600/782], Loss: 0.1525\n",
            "Epoch [8/10], Batch [610/782], Loss: 0.2435\n",
            "Epoch [8/10], Batch [620/782], Loss: 0.3904\n",
            "Epoch [8/10], Batch [630/782], Loss: 0.2572\n",
            "Epoch [8/10], Batch [640/782], Loss: 0.5299\n",
            "Epoch [8/10], Batch [650/782], Loss: 0.1527\n",
            "Epoch [8/10], Batch [660/782], Loss: 0.1932\n",
            "Epoch [8/10], Batch [670/782], Loss: 0.2211\n",
            "Epoch [8/10], Batch [680/782], Loss: 0.2200\n",
            "Epoch [8/10], Batch [690/782], Loss: 0.3076\n",
            "Epoch [8/10], Batch [700/782], Loss: 0.5685\n",
            "Epoch [8/10], Batch [710/782], Loss: 0.6273\n",
            "Epoch [8/10], Batch [720/782], Loss: 0.1558\n",
            "Epoch [8/10], Batch [730/782], Loss: 0.4004\n",
            "Epoch [8/10], Batch [740/782], Loss: 0.2777\n",
            "Epoch [8/10], Batch [750/782], Loss: 0.1986\n",
            "Epoch [8/10], Batch [760/782], Loss: 0.1461\n",
            "Epoch [8/10], Batch [770/782], Loss: 0.5470\n",
            "Epoch [8/10], Batch [780/782], Loss: 0.5549\n",
            "Epoch [8/10], Loss: 0.3016\n",
            "Epoch [9/10], Batch [10/782], Loss: 0.2925\n",
            "Epoch [9/10], Batch [20/782], Loss: 0.0266\n",
            "Epoch [9/10], Batch [30/782], Loss: 0.4635\n",
            "Epoch [9/10], Batch [40/782], Loss: 0.2113\n",
            "Epoch [9/10], Batch [50/782], Loss: 0.4078\n",
            "Epoch [9/10], Batch [60/782], Loss: 0.2812\n",
            "Epoch [9/10], Batch [70/782], Loss: 0.1136\n",
            "Epoch [9/10], Batch [80/782], Loss: 0.1096\n",
            "Epoch [9/10], Batch [90/782], Loss: 0.4043\n",
            "Epoch [9/10], Batch [100/782], Loss: 0.0742\n",
            "Epoch [9/10], Batch [110/782], Loss: 0.1749\n",
            "Epoch [9/10], Batch [120/782], Loss: 0.1881\n",
            "Epoch [9/10], Batch [130/782], Loss: 0.2868\n",
            "Epoch [9/10], Batch [140/782], Loss: 0.4126\n",
            "Epoch [9/10], Batch [150/782], Loss: 0.1792\n",
            "Epoch [9/10], Batch [160/782], Loss: 0.3201\n",
            "Epoch [9/10], Batch [170/782], Loss: 0.2846\n",
            "Epoch [9/10], Batch [180/782], Loss: 0.3386\n",
            "Epoch [9/10], Batch [190/782], Loss: 0.4329\n",
            "Epoch [9/10], Batch [200/782], Loss: 0.3683\n",
            "Epoch [9/10], Batch [210/782], Loss: 0.1047\n",
            "Epoch [9/10], Batch [220/782], Loss: 0.5500\n",
            "Epoch [9/10], Batch [230/782], Loss: 0.3016\n",
            "Epoch [9/10], Batch [240/782], Loss: 0.2227\n",
            "Epoch [9/10], Batch [250/782], Loss: 0.1108\n",
            "Epoch [9/10], Batch [260/782], Loss: 0.2595\n",
            "Epoch [9/10], Batch [270/782], Loss: 0.2792\n",
            "Epoch [9/10], Batch [280/782], Loss: 0.4256\n",
            "Epoch [9/10], Batch [290/782], Loss: 0.1411\n",
            "Epoch [9/10], Batch [300/782], Loss: 0.2107\n",
            "Epoch [9/10], Batch [310/782], Loss: 0.2611\n",
            "Epoch [9/10], Batch [320/782], Loss: 0.1395\n",
            "Epoch [9/10], Batch [330/782], Loss: 0.0625\n",
            "Epoch [9/10], Batch [340/782], Loss: 0.1775\n",
            "Epoch [9/10], Batch [350/782], Loss: 0.5622\n",
            "Epoch [9/10], Batch [360/782], Loss: 0.2436\n",
            "Epoch [9/10], Batch [370/782], Loss: 0.0382\n",
            "Epoch [9/10], Batch [380/782], Loss: 0.4282\n",
            "Epoch [9/10], Batch [390/782], Loss: 0.2003\n",
            "Epoch [9/10], Batch [400/782], Loss: 0.3625\n",
            "Epoch [9/10], Batch [410/782], Loss: 0.4232\n",
            "Epoch [9/10], Batch [420/782], Loss: 0.3221\n",
            "Epoch [9/10], Batch [430/782], Loss: 0.4178\n",
            "Epoch [9/10], Batch [440/782], Loss: 0.4316\n",
            "Epoch [9/10], Batch [450/782], Loss: 0.3931\n",
            "Epoch [9/10], Batch [460/782], Loss: 0.4050\n",
            "Epoch [9/10], Batch [470/782], Loss: 0.2285\n",
            "Epoch [9/10], Batch [480/782], Loss: 0.2377\n",
            "Epoch [9/10], Batch [490/782], Loss: 0.2080\n",
            "Epoch [9/10], Batch [500/782], Loss: 0.4382\n",
            "Epoch [9/10], Batch [510/782], Loss: 0.0557\n",
            "Epoch [9/10], Batch [520/782], Loss: 0.4361\n",
            "Epoch [9/10], Batch [530/782], Loss: 0.2374\n",
            "Epoch [9/10], Batch [540/782], Loss: 0.1229\n",
            "Epoch [9/10], Batch [550/782], Loss: 0.2957\n",
            "Epoch [9/10], Batch [560/782], Loss: 0.7340\n",
            "Epoch [9/10], Batch [570/782], Loss: 0.3968\n",
            "Epoch [9/10], Batch [580/782], Loss: 0.2734\n",
            "Epoch [9/10], Batch [590/782], Loss: 0.2902\n",
            "Epoch [9/10], Batch [600/782], Loss: 0.7074\n",
            "Epoch [9/10], Batch [610/782], Loss: 0.2569\n",
            "Epoch [9/10], Batch [620/782], Loss: 0.1112\n",
            "Epoch [9/10], Batch [630/782], Loss: 0.0633\n",
            "Epoch [9/10], Batch [640/782], Loss: 0.5411\n",
            "Epoch [9/10], Batch [650/782], Loss: 0.3816\n",
            "Epoch [9/10], Batch [660/782], Loss: 0.1249\n",
            "Epoch [9/10], Batch [670/782], Loss: 0.2682\n",
            "Epoch [9/10], Batch [680/782], Loss: 0.1695\n",
            "Epoch [9/10], Batch [690/782], Loss: 0.2439\n",
            "Epoch [9/10], Batch [700/782], Loss: 0.1501\n",
            "Epoch [9/10], Batch [710/782], Loss: 0.5388\n",
            "Epoch [9/10], Batch [720/782], Loss: 0.4550\n",
            "Epoch [9/10], Batch [730/782], Loss: 0.1761\n",
            "Epoch [9/10], Batch [740/782], Loss: 0.7202\n",
            "Epoch [9/10], Batch [750/782], Loss: 0.2184\n",
            "Epoch [9/10], Batch [760/782], Loss: 0.4650\n",
            "Epoch [9/10], Batch [770/782], Loss: 0.1841\n",
            "Epoch [9/10], Batch [780/782], Loss: 0.8222\n",
            "Epoch [9/10], Loss: 0.2984\n",
            "Epoch [10/10], Batch [10/782], Loss: 0.2576\n",
            "Epoch [10/10], Batch [20/782], Loss: 0.3692\n",
            "Epoch [10/10], Batch [30/782], Loss: 0.2025\n",
            "Epoch [10/10], Batch [40/782], Loss: 0.0371\n",
            "Epoch [10/10], Batch [50/782], Loss: 0.4486\n",
            "Epoch [10/10], Batch [60/782], Loss: 0.2706\n",
            "Epoch [10/10], Batch [70/782], Loss: 0.0892\n",
            "Epoch [10/10], Batch [80/782], Loss: 0.3028\n",
            "Epoch [10/10], Batch [90/782], Loss: 0.3079\n",
            "Epoch [10/10], Batch [100/782], Loss: 0.7442\n",
            "Epoch [10/10], Batch [110/782], Loss: 0.4618\n",
            "Epoch [10/10], Batch [120/782], Loss: 0.1890\n",
            "Epoch [10/10], Batch [130/782], Loss: 0.3310\n",
            "Epoch [10/10], Batch [140/782], Loss: 0.2479\n",
            "Epoch [10/10], Batch [150/782], Loss: 0.2820\n",
            "Epoch [10/10], Batch [160/782], Loss: 0.3681\n",
            "Epoch [10/10], Batch [170/782], Loss: 0.1674\n",
            "Epoch [10/10], Batch [180/782], Loss: 0.3845\n",
            "Epoch [10/10], Batch [190/782], Loss: 0.1522\n",
            "Epoch [10/10], Batch [200/782], Loss: 0.2736\n",
            "Epoch [10/10], Batch [210/782], Loss: 0.2335\n",
            "Epoch [10/10], Batch [220/782], Loss: 0.1132\n",
            "Epoch [10/10], Batch [230/782], Loss: 0.2778\n",
            "Epoch [10/10], Batch [240/782], Loss: 0.1222\n",
            "Epoch [10/10], Batch [250/782], Loss: 0.3123\n",
            "Epoch [10/10], Batch [260/782], Loss: 0.3228\n",
            "Epoch [10/10], Batch [270/782], Loss: 0.0896\n",
            "Epoch [10/10], Batch [280/782], Loss: 0.1594\n",
            "Epoch [10/10], Batch [290/782], Loss: 0.2025\n",
            "Epoch [10/10], Batch [300/782], Loss: 0.1611\n",
            "Epoch [10/10], Batch [310/782], Loss: 0.2168\n",
            "Epoch [10/10], Batch [320/782], Loss: 0.5005\n",
            "Epoch [10/10], Batch [330/782], Loss: 0.2694\n",
            "Epoch [10/10], Batch [340/782], Loss: 0.1561\n",
            "Epoch [10/10], Batch [350/782], Loss: 0.3613\n",
            "Epoch [10/10], Batch [360/782], Loss: 0.2654\n",
            "Epoch [10/10], Batch [370/782], Loss: 0.5022\n",
            "Epoch [10/10], Batch [380/782], Loss: 0.1672\n",
            "Epoch [10/10], Batch [390/782], Loss: 0.3816\n",
            "Epoch [10/10], Batch [400/782], Loss: 0.4400\n",
            "Epoch [10/10], Batch [410/782], Loss: 0.2077\n",
            "Epoch [10/10], Batch [420/782], Loss: 0.1771\n",
            "Epoch [10/10], Batch [430/782], Loss: 0.3146\n",
            "Epoch [10/10], Batch [440/782], Loss: 0.5333\n",
            "Epoch [10/10], Batch [450/782], Loss: 0.2193\n",
            "Epoch [10/10], Batch [460/782], Loss: 0.6558\n",
            "Epoch [10/10], Batch [470/782], Loss: 0.6164\n",
            "Epoch [10/10], Batch [480/782], Loss: 0.3674\n",
            "Epoch [10/10], Batch [490/782], Loss: 0.1918\n",
            "Epoch [10/10], Batch [500/782], Loss: 0.4472\n",
            "Epoch [10/10], Batch [510/782], Loss: 0.1602\n",
            "Epoch [10/10], Batch [520/782], Loss: 0.3085\n",
            "Epoch [10/10], Batch [530/782], Loss: 0.3308\n",
            "Epoch [10/10], Batch [540/782], Loss: 0.0634\n",
            "Epoch [10/10], Batch [550/782], Loss: 0.0994\n",
            "Epoch [10/10], Batch [560/782], Loss: 0.3434\n",
            "Epoch [10/10], Batch [570/782], Loss: 0.3002\n",
            "Epoch [10/10], Batch [580/782], Loss: 0.1659\n",
            "Epoch [10/10], Batch [590/782], Loss: 0.1468\n",
            "Epoch [10/10], Batch [600/782], Loss: 0.1119\n",
            "Epoch [10/10], Batch [610/782], Loss: 0.1450\n",
            "Epoch [10/10], Batch [620/782], Loss: 0.2440\n",
            "Epoch [10/10], Batch [630/782], Loss: 0.3600\n",
            "Epoch [10/10], Batch [640/782], Loss: 0.2034\n",
            "Epoch [10/10], Batch [650/782], Loss: 0.2547\n",
            "Epoch [10/10], Batch [660/782], Loss: 0.3765\n",
            "Epoch [10/10], Batch [670/782], Loss: 0.1939\n",
            "Epoch [10/10], Batch [680/782], Loss: 0.2596\n",
            "Epoch [10/10], Batch [690/782], Loss: 0.3780\n",
            "Epoch [10/10], Batch [700/782], Loss: 0.2803\n",
            "Epoch [10/10], Batch [710/782], Loss: 0.6433\n",
            "Epoch [10/10], Batch [720/782], Loss: 0.3462\n",
            "Epoch [10/10], Batch [730/782], Loss: 0.1813\n",
            "Epoch [10/10], Batch [740/782], Loss: 0.4920\n",
            "Epoch [10/10], Batch [750/782], Loss: 0.1929\n",
            "Epoch [10/10], Batch [760/782], Loss: 0.3702\n",
            "Epoch [10/10], Batch [770/782], Loss: 0.1385\n",
            "Epoch [10/10], Batch [780/782], Loss: 0.3196\n",
            "Epoch [10/10], Loss: 0.2663\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "model.eval()  # Set model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():  # No need to track gradients during testing\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()  # Move data to GPU if available\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest score\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy on the test set: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTdGXbiMGDLz",
        "outputId": "8d30f6c0-b819-4350-9348-7da51194177e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 85.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T5vlMwXbeyUX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}